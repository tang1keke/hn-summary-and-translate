<?xml version='1.0' encoding='UTF-8'?>
<?xml-stylesheet type="text/xsl" href="rss-style.xsl"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Hacker News - English</title>
    <link>https://tang1keke.github.io/hn-summary-and-translate</link>
    <description>Hacker News articles summarized and translated to English</description>
    <language>en</language>
    <lastBuildDate>Mon, 10 Nov 2025 03:52:22 +0000</lastBuildDate>
    <generator>HN RSS Translator</generator>
    <atom:link rel="self" type="application/rss+xml" href="https://tang1keke.github.io/hn-summary-and-translate/rss-en.xml"/>
    <item>
      <title>Open source has a growing problem with LLM generated issues</title>
      <link>https://github.com/opencontainers/runc/issues/4990</link>
      <description>&lt;p&gt;My personal opinion is we shouldn't accept anything LLM-generated, but this is probably not the common position of most @opencontainers/runc-maintainers, so we should probably consider LLM-generated code and issues separately. IMHO, we should close all LLM-generated issues as spam, because even if they are describing real issues the entire issue description contains so much unneeded (and probably incorrect) information that it'd be better if they just provided their LLM prompt as an issue instead. , they understand what their patch does and was able to write the code themselves).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=45871531"&gt;&lt;strong&gt;HN Discussion&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href='https://github.com/opencontainers/runc/issues/4990'&gt;Read Original Article&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Top 6 Comments:&lt;/strong&gt;&lt;/p&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;CGamesPlay:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;I make a lot of drive-by contributions, and I use AI coding tools. I submitted my first PR that is a cross between those two recently. It&amp;#x27;s somewhere between &amp;quot;vibe-coded&amp;quot; and &amp;quot;vibe-engineered&amp;quot;, where I definitely read the resulting code, had the agent make multiple revisions, and deployed the result on my own infrastructure before submitting a PR. In the PR I clearly stated that it was done by a coding agent.&lt;p&gt;I can&amp;#x27;t imagine that any policy against LLM code would allow this sort of thing, but I also imagine that if I don&amp;#x27;t say &amp;quot;this was made by a coding agent&amp;quot;, that no one would ever know. So, should I just stop contributing, or start lying?&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;vineyardmike:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;I&amp;#x27;ve seen an uptick in LLM generated bug reports &lt;i&gt;from coworkers&lt;/i&gt;. A employee of my company (but not someone I work with regularly) used one of the CLI LLMs to search through logs for errors, and then automatically cut (hundreds!) of bugs to (sometimes) the correct teams. Turns out it was the result of some manager&amp;#x27;s mandate to &amp;quot;try integrating AI into our workflow&amp;quot;. The resulting email was probably the least professional communication I&amp;#x27;ve ever sent, but the message was received.&lt;p&gt;The only solution I can see is a hard-no policy. If I think this bug is AI, either by content or by reputation, I close without any investigation. If you want it re-opened, you&amp;#x27;ll need to  IRL prove its genuine in an educated, good-faith approach that involves independent efforts to debug.&lt;p&gt;&amp;gt; &amp;quot;If you put your name on AI slop once, I&amp;#x27;ll assume anything with your name on it is (ignorable) slop, so consider if that is professionally advantageous&amp;quot;.&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;lofties:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;Sidenote, but I love that in a GitHub issue discussing banning the use of LLMs, the GitHub interface asks if there&amp;#x27;s anything I&amp;#x27;d like to fix with CoPilot.&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;SoftTalker:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;I think everything has a growing problem with LLM&amp;#x2F;AI generated content. Emails, blog posts, news articles, research papers, grant applications, business proposals, music, art, pretty much everything you can think of.&lt;p&gt;There’s already more human produced content in the world than anyone could ever hope to consume, we don’t need more from AI.&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;dropbox_miner:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;Curious to know if others are seeing a similar uptick in AI slop in issues or PRs for projects they are maintaining. If yes, how are you dealing with this?&lt;p&gt;Some of the software that I maintain is critical to container ecosystem and I&amp;#x27;m an extremely paranoid developer who starts investigating any github issue within a few minutes of it opening. Now, some of these AI slop github issues have a way to &amp;quot;gaslight&amp;quot; me into thinking that some code paths are problematic when they actually are not. And lately AI slop in issues and PRs have been taking up a lot of my time.&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;dropbox_miner:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;There seems to be a bot that routinely creates huge LLM generated Issues on runc github:
&lt;a href="https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;containerd&amp;#x2F;containerd&amp;#x2F;issues&amp;#x2F;12496" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;containerd&amp;#x2F;containerd&amp;#x2F;issues&amp;#x2F;12496&lt;/a&gt;&lt;p&gt;And honestly, its becoming annoying&lt;/p&gt;&lt;/div&gt;</description>
      <guid isPermaLink="true">https://github.com/opencontainers/runc/issues/4990</guid>
      <pubDate>Mon, 10 Nov 2025 02:10:59 +0000</pubDate>
      <comments>https://news.ycombinator.com/item?id=45871531</comments>
    </item>
    <item>
      <title>Itiner-e: the Google Maps of Roman Roads</title>
      <link>https://itiner-e.org/</link>
      <description>&lt;p&gt;Welcome to Itiner-e – The Digital Atlas of Ancient Roads Itiner-e aims to host the most detailed open digital dataset of roads in the entire Roman Empire. The data creation is a collaborative ongoing project edited by a scholarly community. Itiner-e allows you to view, query and download roads.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=45864341"&gt;&lt;strong&gt;HN Discussion&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href='https://itiner-e.org/'&gt;Read Original Article&lt;/a&gt;&lt;/p&gt;</description>
      <guid isPermaLink="true">https://itiner-e.org/</guid>
      <pubDate>Sun, 09 Nov 2025 09:46:04 +0000</pubDate>
      <comments>https://news.ycombinator.com/item?id=45864341</comments>
    </item>
  </channel>
</rss>
