<?xml version='1.0' encoding='UTF-8'?>
<?xml-stylesheet type="text/xsl" href="rss-style.xsl"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Hacker News - Chinese (Simplified)</title>
    <link>https://tang1keke.github.io/hn-summary-and-translate</link>
    <description>Hacker News articles summarized and translated to Chinese (Simplified)</description>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 19 Nov 2025 12:50:08 +0000</lastBuildDate>
    <generator>HN RSS Translator</generator>
    <atom:link rel="self" type="application/rss+xml" href="https://tang1keke.github.io/hn-summary-and-translate/rss-zh.xml"/>
    <item>
      <title>用于思维感知编辑和生成的多模态扩散语言模型</title>
      <link>https://github.com/tyfeld/MMaDA-Parallel</link>
      <description>&lt;p&gt;环境设置 首先，从带有 torch 2 的 torch 环境开始。MMaDA-Parallel-A 使用分词器 Amused-VQ 进行训练，MMaDA-Parallel-M 使用分词器 Magvitv2 进行训练。带有 MMaDA-Parallel-M 的并行生成 cd MMaDA-Parallel-M python 推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original:&lt;/strong&gt; Multimodal Diffusion Language Models for Thinking-Aware Editing and Generation&lt;/p&gt;
&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=45977542"&gt;&lt;strong&gt;HN Discussion&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href='https://github.com/tyfeld/MMaDA-Parallel'&gt;Read Original Article&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Top 3 Comments:&lt;/strong&gt;&lt;/p&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;boriskourt:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;有趣的方法和一篇非常可读的论文。&lt;p&gt;&gt;我们提供两种具有不同分词器的 MMaDA-Parallel 变体。 MMaDA-Parallel-A 使用分词器 Amused-VQ 进行训练，MMaDA-Parallel-M 使用分词器 Magvitv2 进行训练。&lt;p&gt;tyfeld&amp;#x2F;MMaDA-Parallel-A: &lt;a href="https:&amp;#x2F;&amp;#x2F;huggingface.co&amp;#x2F;tyfeld&amp;#x2F;MMaDA-Parallel-A&amp;#x2F;tree&amp;#x2F;main" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;huggingface.co&amp;#x2F;tyfeld&amp;#x2F;MMaDA-Parallel-A&amp;#x2F;tree&amp;#x2F;main&lt;/a&gt;&lt;p&gt;tyfeld&amp;#x2F;MMaDA-Parallel-M: &lt;a href="https:&amp;#x2F;&amp;#x2F;huggingface.co&amp;#x2F;tyfeld&amp;#x2F;MMaDA-Parallel-M&amp;#x2F;tree&amp;#x2F;main" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;huggingface.co&amp;#x2F;tyfeld&amp;#x2F;MMaDA-Parallel-M&amp;#x2F;tree&amp;#x2F;main&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;NitpickLawyer:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;&gt;为了解决这个问题，我们提出了一种并行多模态扩散框架 MMaDA-Parallel，它可以在整个去噪轨迹中实现文本和图像之间的&lt;i&gt;连续、双向交互&lt;/i&gt;。&lt;p&gt;&gt; (ParaRL)，一种新颖的策略，它&lt;i&gt;沿着轨迹应用语义奖励&lt;/i&gt;来强制跨模式一致性。&lt;p&gt;（强调我的）&lt;p&gt;这听起来真的很酷。一代人“参加”这一事实表明了这一点。另一个真的很有趣。我很好奇这是否也适用于其他方式。我正在考虑编写特定的应用程序，一旦生成某些内容，事情就会发生变化。我的预感是，编码将从这种方法中受益匪浅，因为“手动”可以让您受益匪浅。编写代码的方式通常更像扩散而不是自回归（也就是说，我们经常在这里编辑一些东西，然后因为我们这样做了，我们必须导入一些东西，然后在那里改变一些东西，然后导致进一步的改变，等等）。&lt;p&gt;目前，编码似乎从&amp;lt;思考&amp;gt;中受益匪浅。 -&gt; &amp;lt;编码&amp;gt; -&gt; &amp;lt;env_feedback&amp;gt; -&gt; &amp;lt;反思&amp;gt; -&gt; &amp;lt;思考&amp;gt; -&gt; &amp;lt;编码&amp;gt;，但乍一看这似乎是为了自回归生成而硬塞进去的……GPT5 似乎在这方面尤其擅长，具有多个“工具调用”功能。穿插在其思考过程中。我想知道这里提出的并行去噪技术是否会变得更好，其中思考和编码都是并行完成的，并且人们可以“参与”去噪。到另一个。添加一些反馈（linter、编译器、LSP、测试等），这可以发挥作用。如果有效的话。&lt;/p&gt;&lt;/div&gt;
&lt;div style="margin: 10px 0; padding: 10px; background: #f6f6f6; border-left: 3px solid #ff6600;"&gt;&lt;p style="margin: 0 0 5px 0;"&gt;&lt;strong&gt;Hard_Space:&lt;/strong&gt;&lt;/p&gt;&lt;p style="margin: 0;"&gt;请注意，在撰写本文时，项目页面有错误的 Arxiv 链接。这是正确的：&lt;p&gt;&lt;a href="https:&amp;#x2F;&amp;#x2F;arxiv.org&amp;#x2F;abs&amp;#x2F;2511.09611" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;arxiv.org&amp;#x2F;abs&amp;#x2F;2511.09611&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description>
      <guid isPermaLink="true">https://github.com/tyfeld/MMaDA-Parallel</guid>
      <pubDate>Wed, 19 Nov 2025 09:27:17 +0000</pubDate>
      <comments>https://news.ycombinator.com/item?id=45977542</comments>
    </item>
    <item>
      <title>拉皮条的阿米加 500</title>
      <link>https://www.pimyretro.org/pimped-amiga-500/</link>
      <description>&lt;p&gt;早在 90 年代初，我就有一台 Amiga 2000，只有一张扩展卡：一个 SCSI 控制器与一个巨大的 290 MB 硬盘驱动器配对。等一下 — 这是 Amiga 500，而不是 500+。这意味着我的 A500 一定经过修改，将慢速 RAM 转换为芯片 RAM。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original:&lt;/strong&gt; Pimped Amiga 500&lt;/p&gt;
&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=45978545"&gt;&lt;strong&gt;HN Discussion&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href='https://www.pimyretro.org/pimped-amiga-500/'&gt;Read Original Article&lt;/a&gt;&lt;/p&gt;</description>
      <guid isPermaLink="true">https://www.pimyretro.org/pimped-amiga-500/</guid>
      <pubDate>Wed, 19 Nov 2025 12:02:49 +0000</pubDate>
      <comments>https://news.ycombinator.com/item?id=45978545</comments>
    </item>
    <item>
      <title>克拉纳表示，人工智能驱动器已帮助员工人数减半并提高了薪资</title>
      <link>https://www.theguardian.com/business/2025/nov/18/buy-now-pay-later-klarna-ai-helped-halve-staff-boost-pay</link>
      <description>&lt;p&gt;照片：Robert Evans/AlamyKlarna 表示，人工智能的推动已帮助员工人数减少了一半，并提高了薪资。现在购买、稍后支付的公司表示，薪资已经上涨了 60%，而员工人数主要是由于人员流失和技术投资而削减的。Klarna 声称，与人工智能相关的节省使现在购买、稍后支付的公司将员工工资提高了近 60%，但暗示在过去三年将员工数量几乎减半后，该公司可能会削减更多工作岗位。他解释说，克拉纳“已经好几年”没有招聘了。探索有关这些主题的更多信息立即购买，稍后付款失业金融科技人工智能 (AI)计算新闻分享重复使用此内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original:&lt;/strong&gt; Klarna says AI drive has helped halve staff numbers and boost pay&lt;/p&gt;
&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=45978739"&gt;&lt;strong&gt;HN Discussion&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href='https://www.theguardian.com/business/2025/nov/18/buy-now-pay-later-klarna-ai-helped-halve-staff-boost-pay'&gt;Read Original Article&lt;/a&gt;&lt;/p&gt;</description>
      <guid isPermaLink="true">https://www.theguardian.com/business/2025/nov/18/buy-now-pay-later-klarna-ai-helped-halve-staff-boost-pay</guid>
      <pubDate>Wed, 19 Nov 2025 12:25:12 +0000</pubDate>
      <comments>https://news.ycombinator.com/item?id=45978739</comments>
    </item>
  </channel>
</rss>
